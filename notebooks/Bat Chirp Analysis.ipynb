{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b44c5b-ec53-41b3-8265-0903deb9b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Use Apple Silicon M3 chip\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe05d21",
   "metadata": {},
   "source": [
    "First, we will limit our analysis to a single species for simplicity. In this case, we will choose the Coto species (commonly known as Townsend's big-eared bat), since it is the species with the largest number of examples in our dataset (n = 116696)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2413817-958b-46ca-b7c5-8bb5f4652526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_daytime value counts:\n",
      "is_daytime\n",
      "False    111383\n",
      "True       5313\n",
      "Name: count, dtype: int64\n",
      "\n",
      "False (nighttime): 111383 (95.45%)\n",
      "True (daytime): 5313 (4.55%)\n",
      "\n",
      "Dataframe shape: (116696, 111)\n",
      "Number of feature columns (excluding TimeInFile and file_id): 109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_feather(\"src/data/orig_chirps_2024-06-25T12_55_03.feather\")\n",
    "df_Coto = df[df['species'] == 'Coto'].copy()\n",
    "\n",
    "# Check is_daytime distribution\n",
    "print(\"is_daytime value counts:\")\n",
    "daytime_counts = df_Coto['is_daytime'].value_counts()\n",
    "print(daytime_counts)\n",
    "print(f\"\\nFalse (nighttime): {daytime_counts.get(False, 0)} ({daytime_counts.get(False, 0) / len(df_Coto) * 100:.2f}%)\")\n",
    "print(f\"True (daytime): {daytime_counts.get(True, 0)} ({daytime_counts.get(True, 0) / len(df_Coto) * 100:.2f}%)\")\n",
    "\n",
    "# Drop redundant/non-feature columns:\n",
    "# - species: all same (Coto)\n",
    "# - TimeInFile: will be used for edge features only\n",
    "# - PrecedingIntrvl: redundant with TimeInFile differences\n",
    "# - CallsPerSec: constant within recording\n",
    "# - file_id, chirp_idx, split, rec_datetime: metadata, not chirp features\n",
    "# - MinAccpQuality, Max#CallsConsidered, cntxt_sz: processing parameters\n",
    "# - sin_year, cos_year: too coarse temporal info\n",
    "df_Coto = df_Coto.drop(['species', 'PrecedingIntrvl', 'CallsPerSec', 'chirp_idx', 'split', 'rec_datetime', 'MinAccpQuality', 'Max#CallsConsidered', 'cntxt_sz', 'sin_year', 'cos_year'], axis=1)\n",
    "print(f\"\\nDataframe shape: {df_Coto.shape}\")\n",
    "print(f\"Number of feature columns (excluding TimeInFile and file_id): {len(df_Coto.columns) - 2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801f772",
   "metadata": {},
   "source": [
    "The original dataset is a compilation of a number of recordings taken at different dates and times. We want to analyze only within each recording, since it is unreasonable to assume that there should be any causal connection between bat chirps which are not temporally connected. We also will filter out all recordings which occur during the daytime, since the context for those chirps is different from chirps which occur at night. This brings us down from 5350 files to 5080 files, and from 116696 total chirps to 111383 total chirps. \n",
    "\n",
    "\n",
    "After splitting by recording and removing daytime recordings, we create individual graphs for each recording, which are indexed with the variable 'file_id.' This yields a total of 5350 unique graphs, with mean size of around 22 chirps, and a minimum and maximum of 6 and 54 chirps respectively. Clusters will be compared across graphs to determine patterns which occur within the species. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "822d7653-e885-4dc6-8a23-32b0103a5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique file_ids: 5350\n",
      "File_ids with all nighttime chirps: 5080\n",
      "\n",
      "Total chirps before filtering: 116696\n",
      "Total chirps after filtering: 111383\n",
      "\n",
      " Number of nighttime recordings = 5080\n",
      "Min chirps per recording: 6\n",
      "Max chirps per recording: 54\n",
      "Mean chirps per recording: 21.93\n"
     ]
    }
   ],
   "source": [
    "# Group by file_id and check if all chirps in each file are nighttime\n",
    "file_id_all_nighttime = df_Coto.groupby('file_id')['is_daytime'].apply(lambda x: (~x).all())\n",
    "nighttime_file_ids = file_id_all_nighttime[file_id_all_nighttime].index.tolist()\n",
    "\n",
    "print(f\"Total unique file_ids: {df_Coto['file_id'].nunique()}\")\n",
    "print(f\"File_ids with all nighttime chirps: {len(nighttime_file_ids)}\")\n",
    "\n",
    "# Filter df_Coto to only include nighttime file_ids\n",
    "df_Coto_filtered = df_Coto[df_Coto['file_id'].isin(nighttime_file_ids)].copy()\n",
    "df_Coto_filtered = df_Coto_filtered.drop(['is_daytime'], axis=1)\n",
    "print(f\"\\nTotal chirps before filtering: {len(df_Coto)}\")\n",
    "print(f\"Total chirps after filtering: {len(df_Coto_filtered)}\")\n",
    "\n",
    "# Split df_Coto_filtered into separate dataframes by file_id\n",
    "unique_file_ids = df_Coto_filtered['file_id'].unique()\n",
    "n = len(unique_file_ids)\n",
    "\n",
    "# Create dictionary to store dataframes\n",
    "dataframes = {}\n",
    "lengths = []\n",
    "file_id_to_index = {}\n",
    "\n",
    "# Split dataframe by file_id and store each\n",
    "for i, file_id in enumerate(unique_file_ids):\n",
    "    df_name = f'df_{i}'\n",
    "    dataframes[df_name] = df_Coto_filtered[df_Coto_filtered['file_id'] == file_id].copy()\n",
    "    lengths.append(len(dataframes[df_name]))\n",
    "    file_id_to_index[file_id] = i\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n Number of nighttime recordings = {n}\")\n",
    "print(f\"Min chirps per recording: {min(lengths)}\")\n",
    "print(f\"Max chirps per recording: {max(lengths)}\")\n",
    "print(f\"Mean chirps per recording: {np.mean(np.array(lengths)):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b148cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5080 graphs\n",
      "Example graph 0:\n",
      "  - Number of nodes: 18\n",
      "  - Number of edges: 17\n",
      "  - Node feature dimension: 108\n",
      "  - Edge feature dimension: 1\n",
      "  - File ID: 39\n",
      "\n",
      "Example graph 10:\n",
      "  - Number of nodes: 19\n",
      "  - Number of edges: 18\n",
      "  - Node feature dimension: 108\n",
      "  - Edge feature dimension: 1\n",
      "  - First 3 edge features (time diffs in ms): [92.0, 68.0, 57.0]\n"
     ]
    }
   ],
   "source": [
    "# Create 1D graphs for each recording (file_id)\n",
    "# Each graph is a chain where nodes are chirps and edges connect consecutive chirps\n",
    "graphs = []  \n",
    "\n",
    "for i in range(n):\n",
    "    df_name = f'df_{i}'\n",
    "    df_current = dataframes[df_name]\n",
    "    \n",
    "    # Get number of nodes (chirps) in this graph\n",
    "    num_nodes = len(df_current)\n",
    "    \n",
    "    # Extract node features (all columns except TimeInFile, file_id, and any non-numeric columns)\n",
    "    feature_cols = [col for col in df_current.columns \n",
    "                    if col not in ['TimeInFile', 'file_id']]\n",
    "    node_features = torch.tensor(df_current[feature_cols].values, dtype=torch.float32)\n",
    "    \n",
    "    # Create sequential edge indices for 1D chain graph\n",
    "    # Edge from node i to node i+1 for all i in [0, num_nodes-2]\n",
    "    if num_nodes > 1:\n",
    "        edge_index = torch.tensor([\n",
    "            list(range(num_nodes - 1)),  # source nodes\n",
    "            list(range(1, num_nodes))     # target nodes\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        # Calculate edge features: time difference between consecutive chirps\n",
    "        time_values = df_current['TimeInFile'].values\n",
    "        time_diffs = time_values[1:] - time_values[:-1]\n",
    "        edge_attr = torch.tensor(time_diffs, dtype=torch.float32).unsqueeze(1)  # Shape: [num_edges, 1]\n",
    "    else:\n",
    "        # Single node graph - no edges\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, 1), dtype=torch.float32)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    graph = Data(\n",
    "        x=node_features,           # Node features\n",
    "        edge_index=edge_index,     # Edge connectivity\n",
    "        edge_attr=edge_attr,       # Edge features (time differences)\n",
    "        num_nodes=num_nodes\n",
    "    )\n",
    "    \n",
    "    # Store original file_id as graph attribute\n",
    "    graph.file_id = df_current['file_id'].iloc[0]\n",
    "    \n",
    "    graphs.append(graph)\n",
    "\n",
    "print(f\"Created {len(graphs)} graphs\")\n",
    "print(f\"Example graph 0:\")\n",
    "print(f\"  - Number of nodes: {graphs[0].num_nodes}\")\n",
    "print(f\"  - Number of edges: {graphs[0].edge_index.shape[1]}\")\n",
    "print(f\"  - Node feature dimension: {graphs[0].x.shape[1]}\")\n",
    "print(f\"  - Edge feature dimension: {graphs[0].edge_attr.shape[1]}\")\n",
    "print(f\"  - File ID: {graphs[0].file_id}\")\n",
    "print(f\"\\nExample graph 10:\")\n",
    "print(f\"  - Number of nodes: {graphs[10].num_nodes}\")\n",
    "print(f\"  - Number of edges: {graphs[10].edge_index.shape[1]}\")\n",
    "print(f\"  - Node feature dimension: {graphs[10].x.shape[1]}\")\n",
    "print(f\"  - Edge feature dimension: {graphs[0].edge_attr.shape[1]}\")\n",
    "print(f\"  - First 3 edge features (time diffs in ms): {graphs[10].edge_attr[:3].squeeze().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a746f35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building hybrid graphs with temporal + k-NN edges...\n",
      "Original graph 0: 17 edges\n",
      "Hybrid graph 0: 107 edges\n",
      "Edge increase: 90 k-NN edges added\n",
      "\n",
      "Initializing GraphSAGE model...\n",
      "Model parameters: 18016\n",
      "\n",
      "Preparing contrastive learning dataset...\n",
      "Created 212606 training pairs (106303 positive, 106303 negative)\n",
      "\n",
      "Training GraphSAGE model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [MPSFloatType [32, 64]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 232\u001b[39m\n\u001b[32m    229\u001b[39m loss = criterion(scores, batch_labels)\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m optimizer.step()\n\u001b[32m    235\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BatGPT/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BatGPT/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BatGPT/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: one of the variables needed for gradient computation has been modified by an inplace operation: [MPSFloatType [32, 64]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GraphSAGE Implementation for Chirp Cluster Identification\n",
    "\n",
    "ARCHITECTURE OVERVIEW:\n",
    "======================\n",
    "\n",
    "1. GRAPH CONSTRUCTION (Hybrid Approach):\n",
    "   - Temporal edges: Connect consecutive chirps in time sequence\n",
    "   - k-NN edges: Connect each chirp to k=5 nearest neighbors based on feature similarity\n",
    "   - Edge features: Time differences for temporal edges, feature distances for k-NN edges\n",
    "   \n",
    "2. GRAPHSAGE MODEL:\n",
    "   - Input dimension: 108 (chirp features)\n",
    "   - Hidden dimension: 64\n",
    "   - Output dimension: 32 (embedding space)\n",
    "   - Number of layers: 2\n",
    "   - Aggregation: Mean aggregation (robust to varying neighborhood sizes)\n",
    "   - Activation: ReLU\n",
    "   - Dropout: 0.2 (regularization)\n",
    "   \n",
    "3. TRAINING STRATEGY (Self-Supervised):\n",
    "   - Contrastive learning: Predict whether chirp pairs are temporally adjacent\n",
    "   - Positive pairs: Consecutive chirps in same recording\n",
    "   - Negative pairs: Random chirps from same or different recordings\n",
    "   - Loss: Binary cross-entropy\n",
    "   - Optimizer: Adam with lr=0.001\n",
    "   - Epochs: 50-100 (should complete in 5-10 minutes)\n",
    "   \n",
    "4. CLUSTERING APPROACH:\n",
    "   - Algorithm: HDBSCAN (hierarchical density-based clustering)\n",
    "   - Benefits: Automatically determines number of clusters, handles noise\n",
    "   - Applied to: Node embeddings from trained GraphSAGE\n",
    "   \n",
    "5. CROSS-RECORDING ANALYSIS:\n",
    "   - Extract cluster distributions per recording (graph-level signatures)\n",
    "   - Compute cluster co-occurrence matrix across recordings\n",
    "   - Identify common \"sentence\" patterns that appear across multiple recordings\n",
    "\"\"\"\n",
    "\n",
    "print(\"Building hybrid graphs with temporal + k-NN edges...\")\n",
    "\n",
    "# Step 1: Rebuild graphs with k-NN edges\n",
    "def add_knn_edges(graph, k=5):\n",
    "    \"\"\"Add k-NN edges based on feature similarity to existing temporal edges\"\"\"\n",
    "    x = graph.x.numpy()\n",
    "    \n",
    "    # Normalize features for better distance computation\n",
    "    scaler = StandardScaler()\n",
    "    x_normalized = scaler.fit_transform(x)\n",
    "    \n",
    "    # Find k nearest neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k+1, len(x)), algorithm='ball_tree').fit(x_normalized)\n",
    "    distances, indices = nbrs.kneighbors(x_normalized)\n",
    "    \n",
    "    # Build k-NN edge list (skip first neighbor which is the node itself)\n",
    "    knn_edges = []\n",
    "    knn_edge_features = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        for j, neighbor_idx in enumerate(indices[i][1:]):  # Skip self\n",
    "            knn_edges.append([i, neighbor_idx])\n",
    "            # Use normalized distance as edge feature\n",
    "            knn_edge_features.append([distances[i][j+1]])\n",
    "    \n",
    "    if len(knn_edges) > 0:\n",
    "        knn_edge_index = torch.tensor(knn_edges, dtype=torch.long).t()\n",
    "        knn_edge_attr = torch.tensor(knn_edge_features, dtype=torch.float32)\n",
    "        \n",
    "        # Combine temporal and k-NN edges\n",
    "        combined_edge_index = torch.cat([graph.edge_index, knn_edge_index], dim=1)\n",
    "        \n",
    "        # Normalize temporal edge features to similar scale as k-NN distances\n",
    "        temporal_edge_norm = graph.edge_attr / graph.edge_attr.max() if graph.edge_attr.numel() > 0 else graph.edge_attr\n",
    "        combined_edge_attr = torch.cat([temporal_edge_norm, knn_edge_attr], dim=0)\n",
    "        \n",
    "        graph.edge_index = combined_edge_index\n",
    "        graph.edge_attr = combined_edge_attr\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Add k-NN edges to all graphs\n",
    "hybrid_graphs = [add_knn_edges(graph.clone(), k=5) for graph in graphs]\n",
    "\n",
    "print(f\"Original graph 0: {graphs[0].edge_index.shape[1]} edges\")\n",
    "print(f\"Hybrid graph 0: {hybrid_graphs[0].edge_index.shape[1]} edges\")\n",
    "print(f\"Edge increase: {hybrid_graphs[0].edge_index.shape[1] - graphs[0].edge_index.shape[1]} k-NN edges added\")\n",
    "print()\n",
    "\n",
    "# Step 2: Define GraphSAGE Model\n",
    "class ChirpGraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE model for learning chirp embeddings\n",
    "    \n",
    "    Architecture:\n",
    "    - Layer 1: SAGEConv(108 -> 64) + ReLU + Dropout(0.2)\n",
    "    - Layer 2: SAGEConv(64 -> 32)\n",
    "    \n",
    "    The mean aggregation combines features from temporal and k-NN neighbors,\n",
    "    allowing the model to learn both sequential patterns and acoustic similarities.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(ChirpGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1: Aggregate from 1-hop neighbors\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 2: Aggregate from 2-hop neighbors\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get final node embeddings (without dropout for inference)\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x, edge_index)\n",
    "\n",
    "print(\"Initializing GraphSAGE model...\")\n",
    "model = ChirpGraphSAGE(in_channels=108, hidden_channels=64, out_channels=32, dropout=0.2)\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Prepare training data for contrastive learning\n",
    "print(\"Preparing contrastive learning dataset...\")\n",
    "\n",
    "def create_training_pairs(graphs, num_negatives_per_positive=1):\n",
    "    \"\"\"\n",
    "    Create positive and negative pairs for contrastive learning\n",
    "    Positive: temporally adjacent chirps\n",
    "    Negative: random non-adjacent chirps\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    graph_indices = []\n",
    "    \n",
    "    for graph_idx, graph in enumerate(graphs):\n",
    "        num_nodes = graph.num_nodes\n",
    "        \n",
    "        if num_nodes < 2:\n",
    "            continue\n",
    "        \n",
    "        # Positive pairs: consecutive chirps (from temporal edges only)\n",
    "        temporal_edges = graphs[graph_idx].edge_index.shape[1]  # Original temporal edges\n",
    "        for i in range(min(temporal_edges, num_nodes - 1)):\n",
    "            pairs.append([i, i + 1])\n",
    "            labels.append(1)\n",
    "            graph_indices.append(graph_idx)\n",
    "            \n",
    "            # Add negatives: random non-adjacent pairs\n",
    "            for _ in range(num_negatives_per_positive):\n",
    "                node1 = np.random.randint(0, num_nodes)\n",
    "                # Sample node2 that's not adjacent to node1\n",
    "                possible_nodes = [j for j in range(num_nodes) if abs(j - node1) > 1]\n",
    "                if len(possible_nodes) > 0:\n",
    "                    node2 = np.random.choice(possible_nodes)\n",
    "                    pairs.append([node1, node2])\n",
    "                    labels.append(0)\n",
    "                    graph_indices.append(graph_idx)\n",
    "    \n",
    "    return pairs, labels, graph_indices\n",
    "\n",
    "pairs, labels, graph_indices = create_training_pairs(hybrid_graphs, num_negatives_per_positive=1)\n",
    "print(f\"Created {len(pairs)} training pairs ({sum(labels)} positive, {len(labels) - sum(labels)} negative)\")\n",
    "print()\n",
    "\n",
    "# Step 4: Training loop (OPTIMIZED - pre-compute embeddings per epoch)\n",
    "print(\"Training GraphSAGE model...\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 1024  # Increased batch size for efficiency\n",
    "\n",
    "# Convert pairs and labels to tensors for faster processing\n",
    "pairs_tensor = torch.tensor(pairs, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "graph_indices_tensor = torch.tensor(graph_indices, dtype=torch.long)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # OPTIMIZATION: Pre-compute embeddings for all graphs once per epoch\n",
    "    all_graph_embeddings = []\n",
    "    for graph in hybrid_graphs:\n",
    "        graph = graph.to(device)\n",
    "        embeddings = model(graph.x, graph.edge_index)\n",
    "        all_graph_embeddings.append(embeddings)\n",
    "    \n",
    "    # Shuffle training data\n",
    "    indices = torch.randperm(len(pairs))\n",
    "    \n",
    "    for batch_start in range(0, len(pairs), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_indices = indices[batch_start:batch_start + batch_size]\n",
    "        batch_pairs = pairs_tensor[batch_indices]\n",
    "        batch_labels = labels_tensor[batch_indices].to(device)\n",
    "        batch_graph_indices = graph_indices_tensor[batch_indices]\n",
    "        \n",
    "        # Compute scores for all pairs in batch\n",
    "        scores = []\n",
    "        for i, idx in enumerate(batch_indices):\n",
    "            graph_idx = batch_graph_indices[i].item()\n",
    "            node1, node2 = batch_pairs[i]\n",
    "            \n",
    "            # Look up pre-computed embeddings\n",
    "            emb1 = all_graph_embeddings[graph_idx][node1]\n",
    "            emb2 = all_graph_embeddings[graph_idx][node2]\n",
    "            score = torch.sum(emb1 * emb2)\n",
    "            scores.append(score)\n",
    "        \n",
    "        scores = torch.stack(scores)\n",
    "        \n",
    "        # Compute loss for entire batch\n",
    "        loss = criterion(scores, batch_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Time: {elapsed:.1f}s\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GraphSAGE Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerating embeddings for all chirps...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate embeddings for all chirps\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_graph_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for graph_idx, graph in enumerate(hybrid_graphs):\n",
    "        graph = graph.to(device)\n",
    "        embeddings = model(graph.x, graph.edge_index)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_graph_ids.extend([graph_idx] * graph.num_nodes)\n",
    "\n",
    "# Concatenate all embeddings\n",
    "all_embeddings_concat = np.vstack(all_embeddings)\n",
    "all_graph_ids = np.array(all_graph_ids)\n",
    "\n",
    "print(f\"Generated embeddings for {len(all_embeddings_concat)} chirps\")\n",
    "print(f\"Embedding shape: {all_embeddings_concat.shape}\")\n",
    "print()\n",
    "\n",
    "# Step 6: Apply HDBSCAN clustering\n",
    "print(\"Applying HDBSCAN clustering...\")\n",
    "print(\"HDBSCAN Parameters:\")\n",
    "print(\"  - min_cluster_size: 50 (minimum chirps to form a cluster)\")\n",
    "print(\"  - min_samples: 10 (neighborhood size for density estimation)\")\n",
    "print(\"  - metric: euclidean\")\n",
    "print()\n",
    "\n",
    "clusterer = HDBSCAN(min_cluster_size=50, min_samples=10, metric='euclidean')\n",
    "cluster_labels = clusterer.fit_predict(all_embeddings_concat)\n",
    "\n",
    "# Analyze clustering results\n",
    "unique_clusters = set(cluster_labels)\n",
    "num_clusters = len(unique_clusters - {-1})  # Exclude noise (-1)\n",
    "num_noise = sum(cluster_labels == -1)\n",
    "\n",
    "print(f\"Clustering Results:\")\n",
    "print(f\"  - Number of clusters found: {num_clusters}\")\n",
    "print(f\"  - Noise points (unassigned): {num_noise} ({num_noise/len(cluster_labels)*100:.2f}%)\")\n",
    "print(f\"  - Clustered points: {len(cluster_labels) - num_noise} ({(len(cluster_labels)-num_noise)/len(cluster_labels)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Cluster size distribution\n",
    "cluster_sizes = Counter(cluster_labels)\n",
    "del cluster_sizes[-1]  # Remove noise\n",
    "cluster_sizes_sorted = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 largest clusters:\")\n",
    "for cluster_id, size in cluster_sizes_sorted[:10]:\n",
    "    print(f\"  Cluster {cluster_id}: {size} chirps ({size/len(cluster_labels)*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Visualize cluster size distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sizes = [size for _, size in cluster_sizes_sorted]\n",
    "plt.hist(sizes, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Cluster Size (number of chirps)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Cluster Size Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(min(20, len(cluster_sizes_sorted))), [s for _, s in cluster_sizes_sorted[:20]])\n",
    "plt.xlabel('Cluster ID (top 20)')\n",
    "plt.ylabel('Number of Chirps')\n",
    "plt.title('Top 20 Clusters by Size')\n",
    "plt.xticks(range(min(20, len(cluster_sizes_sorted))), [c for c, _ in cluster_sizes_sorted[:20]], rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComputing graph-level signatures (cluster distributions per recording)...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36817e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Graph-level signature analysis\n",
    "# For each recording, compute which clusters appear and how often\n",
    "\n",
    "recording_cluster_signatures = defaultdict(lambda: defaultdict(int))\n",
    "cluster_to_recordings = defaultdict(set)\n",
    "\n",
    "# Build cluster signatures for each recording\n",
    "idx = 0\n",
    "for graph_idx, graph in enumerate(hybrid_graphs):\n",
    "    num_nodes = graph.num_nodes\n",
    "    graph_cluster_labels = cluster_labels[idx:idx + num_nodes]\n",
    "    \n",
    "    for cluster_id in graph_cluster_labels:\n",
    "        if cluster_id != -1:  # Ignore noise\n",
    "            recording_cluster_signatures[graph_idx][cluster_id] += 1\n",
    "            cluster_to_recordings[cluster_id].add(graph_idx)\n",
    "    \n",
    "    idx += num_nodes\n",
    "\n",
    "print(f\"Built signatures for {len(recording_cluster_signatures)} recordings\")\n",
    "print()\n",
    "\n",
    "# Analyze cluster prevalence across recordings\n",
    "print(\"Cluster Prevalence Analysis:\")\n",
    "print(\"How many recordings contain each cluster (i.e., how common is each 'sentence')?\")\n",
    "print()\n",
    "\n",
    "cluster_prevalence = [(cluster_id, len(recordings)) \n",
    "                      for cluster_id, recordings in cluster_to_recordings.items()]\n",
    "cluster_prevalence_sorted = sorted(cluster_prevalence, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 15 most common clusters (appear in most recordings):\")\n",
    "for cluster_id, num_recordings in cluster_prevalence_sorted[:15]:\n",
    "    cluster_size = cluster_sizes[cluster_id]\n",
    "    print(f\"  Cluster {cluster_id}: appears in {num_recordings} recordings ({num_recordings/len(recording_cluster_signatures)*100:.2f}%), \"\n",
    "          f\"total {cluster_size} chirps\")\n",
    "print()\n",
    "\n",
    "# Find recordings with similar cluster profiles\n",
    "print(\"Finding recordings with similar cluster profiles...\")\n",
    "\n",
    "# Convert signatures to vectors for similarity computation\n",
    "all_cluster_ids = sorted(set(cluster_labels) - {-1})\n",
    "cluster_id_to_idx = {cid: i for i, cid in enumerate(all_cluster_ids)}\n",
    "\n",
    "signature_vectors = []\n",
    "signature_graph_ids = []\n",
    "\n",
    "for graph_idx in recording_cluster_signatures:\n",
    "    vec = np.zeros(len(all_cluster_ids))\n",
    "    for cluster_id, count in recording_cluster_signatures[graph_idx].items():\n",
    "        vec[cluster_id_to_idx[cluster_id]] = count\n",
    "    # Normalize by total chirps in recording to get proportions\n",
    "    vec = vec / vec.sum() if vec.sum() > 0 else vec\n",
    "    signature_vectors.append(vec)\n",
    "    signature_graph_ids.append(graph_idx)\n",
    "\n",
    "signature_vectors = np.array(signature_vectors)\n",
    "print(f\"Created signature vectors of dimension {signature_vectors.shape[1]} for {len(signature_vectors)} recordings\")\n",
    "print()\n",
    "\n",
    "# Compute similarity between recordings (cosine similarity)\n",
    "similarity_matrix = cosine_similarity(signature_vectors)\n",
    "\n",
    "# Find pairs of recordings with high similarity (excluding self-similarity)\n",
    "print(\"Most similar recording pairs (share similar cluster patterns):\")\n",
    "similarity_pairs = []\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(i + 1, len(similarity_matrix)):\n",
    "        similarity_pairs.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "similarity_pairs_sorted = sorted(similarity_pairs, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 most similar recording pairs:\")\n",
    "for i, j, sim in similarity_pairs_sorted[:10]:\n",
    "    graph_i = signature_graph_ids[i]\n",
    "    graph_j = signature_graph_ids[j]\n",
    "    file_id_i = hybrid_graphs[graph_i].file_id\n",
    "    file_id_j = hybrid_graphs[graph_j].file_id\n",
    "    \n",
    "    # Find shared clusters\n",
    "    clusters_i = set(recording_cluster_signatures[graph_i].keys())\n",
    "    clusters_j = set(recording_cluster_signatures[graph_j].keys())\n",
    "    shared_clusters = clusters_i & clusters_j\n",
    "    \n",
    "    print(f\"  Recording {graph_i} (file {file_id_i}) <-> Recording {graph_j} (file {file_id_j})\")\n",
    "    print(f\"    Similarity: {sim:.3f}, Shared clusters: {len(shared_clusters)}/{len(clusters_i | clusters_j)}\")\n",
    "print()\n",
    "\n",
    "# Visualize similarity matrix (sample)\n",
    "print(\"Visualizing recording similarity matrix (first 100 recordings)...\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "sample_size = min(100, len(similarity_matrix))\n",
    "plt.imshow(similarity_matrix[:sample_size, :sample_size], cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xlabel('Recording Index')\n",
    "plt.ylabel('Recording Index')\n",
    "plt.title(f'Recording Similarity Matrix (first {sample_size} recordings)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster co-occurrence\n",
    "print(\"\\nCluster Co-occurrence Analysis:\")\n",
    "print(\"Which clusters tend to appear together in the same recordings?\")\n",
    "print()\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "cluster_cooccurrence = np.zeros((len(all_cluster_ids), len(all_cluster_ids)))\n",
    "\n",
    "for graph_idx in recording_cluster_signatures:\n",
    "    clusters_in_recording = list(recording_cluster_signatures[graph_idx].keys())\n",
    "    for i, c1 in enumerate(clusters_in_recording):\n",
    "        for c2 in clusters_in_recording[i:]:\n",
    "            idx1 = cluster_id_to_idx[c1]\n",
    "            idx2 = cluster_id_to_idx[c2]\n",
    "            cluster_cooccurrence[idx1, idx2] += 1\n",
    "            if idx1 != idx2:\n",
    "                cluster_cooccurrence[idx2, idx1] += 1\n",
    "\n",
    "# Normalize by cluster prevalence to get co-occurrence strength\n",
    "for i in range(len(all_cluster_ids)):\n",
    "    for j in range(len(all_cluster_ids)):\n",
    "        cluster_i = all_cluster_ids[i]\n",
    "        cluster_j = all_cluster_ids[j]\n",
    "        max_possible = min(len(cluster_to_recordings[cluster_i]), \n",
    "                          len(cluster_to_recordings[cluster_j]))\n",
    "        if max_possible > 0:\n",
    "            cluster_cooccurrence[i, j] /= max_possible\n",
    "\n",
    "# Find strongest co-occurrences\n",
    "print(\"Strongest cluster co-occurrences (top 15):\")\n",
    "cooccurrence_pairs = []\n",
    "for i in range(len(all_cluster_ids)):\n",
    "    for j in range(i + 1, len(all_cluster_ids)):\n",
    "        cooccurrence_pairs.append((all_cluster_ids[i], all_cluster_ids[j], \n",
    "                                  cluster_cooccurrence[i, j]))\n",
    "\n",
    "cooccurrence_pairs_sorted = sorted(cooccurrence_pairs, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for c1, c2, strength in cooccurrence_pairs_sorted[:15]:\n",
    "    recordings_with_both = len(cluster_to_recordings[c1] & cluster_to_recordings[c2])\n",
    "    print(f\"  Clusters {c1} & {c2}: co-occur in {recordings_with_both} recordings, \"\n",
    "          f\"strength = {strength:.3f}\")\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY OF FINDINGS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total recordings analyzed: {len(recording_cluster_signatures)}\")\n",
    "print(f\"Total clusters discovered: {num_clusters}\")\n",
    "print(f\"Average clusters per recording: {np.mean([len(sig) for sig in recording_cluster_signatures.values()]):.2f}\")\n",
    "print(f\"Max clusters in a single recording: {max([len(sig) for sig in recording_cluster_signatures.values()])}\")\n",
    "print(f\"Min clusters in a single recording: {min([len(sig) for sig in recording_cluster_signatures.values()])}\")\n",
    "print()\n",
    "print(f\"Most prevalent cluster (Cluster {cluster_prevalence_sorted[0][0]}): \"\n",
    "      f\"appears in {cluster_prevalence_sorted[0][1]} recordings \"\n",
    "      f\"({cluster_prevalence_sorted[0][1]/len(recording_cluster_signatures)*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - Each cluster represents a distinct 'sentence' pattern in bat communication\")\n",
    "print(\"  - Recordings with high similarity share similar communication patterns\")\n",
    "print(\"  - Prevalent clusters appearing across many recordings suggest\")\n",
    "print(\"    common communication behaviors across the species\")\n",
    "print(\"  - Co-occurring clusters may represent sequential patterns or context-dependent communication\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for exploring clusters\n",
    "\n",
    "def visualize_cluster_embeddings(cluster_id, method='tsne'):\n",
    "    \"\"\"\n",
    "    Visualize embeddings of chirps in a specific cluster\n",
    "    \n",
    "    Args:\n",
    "        cluster_id: The cluster to visualize\n",
    "        method: 'tsne' or 'pca' for dimensionality reduction\n",
    "    \"\"\"\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_embeddings = all_embeddings_concat[cluster_mask]\n",
    "    \n",
    "    if len(cluster_embeddings) < 2:\n",
    "        print(f\"Cluster {cluster_id} has too few points to visualize\")\n",
    "        return\n",
    "    \n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "        reduced = reducer.fit_transform(cluster_embeddings)\n",
    "        title = f'Cluster {cluster_id} - t-SNE Visualization'\n",
    "    else:  # pca\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        reduced = reducer.fit_transform(cluster_embeddings)\n",
    "        title = f'Cluster {cluster_id} - PCA Visualization'\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], alpha=0.5, s=20)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Cluster {cluster_id} contains {len(cluster_embeddings)} chirps\")\n",
    "    print(f\"Appears in {len(cluster_to_recordings[cluster_id])} recordings\")\n",
    "\n",
    "def get_cluster_statistics(cluster_id):\n",
    "    \"\"\"Get detailed statistics about a specific cluster\"\"\"\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_indices = np.where(cluster_mask)[0]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} STATISTICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total chirps in cluster: {len(cluster_indices)}\")\n",
    "    print(f\"Appears in {len(cluster_to_recordings[cluster_id])} recordings\")\n",
    "    print(f\"Prevalence: {len(cluster_to_recordings[cluster_id])/len(recording_cluster_signatures)*100:.2f}% of recordings\")\n",
    "    \n",
    "    # Find which recordings contain this cluster\n",
    "    recordings_with_cluster = sorted(list(cluster_to_recordings[cluster_id]))[:10]\n",
    "    print(f\"\\nExample recordings containing this cluster (first 10):\")\n",
    "    for rec_idx in recordings_with_cluster:\n",
    "        file_id = hybrid_graphs[rec_idx].file_id\n",
    "        num_chirps_in_cluster = recording_cluster_signatures[rec_idx][cluster_id]\n",
    "        total_chirps = hybrid_graphs[rec_idx].num_nodes\n",
    "        print(f\"  Recording {rec_idx} (file {file_id}): {num_chirps_in_cluster}/{total_chirps} chirps \"\n",
    "              f\"({num_chirps_in_cluster/total_chirps*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze temporal distribution within recordings\n",
    "    print(f\"\\nTemporal distribution:\")\n",
    "    temporal_positions = []\n",
    "    for rec_idx in cluster_to_recordings[cluster_id]:\n",
    "        # Find positions of cluster chirps within this recording\n",
    "        start_idx = sum(hybrid_graphs[i].num_nodes for i in range(rec_idx))\n",
    "        end_idx = start_idx + hybrid_graphs[rec_idx].num_nodes\n",
    "        rec_cluster_mask = cluster_labels[start_idx:end_idx] == cluster_id\n",
    "        positions = np.where(rec_cluster_mask)[0]\n",
    "        # Normalize positions to [0, 1]\n",
    "        normalized_positions = positions / len(rec_cluster_mask) if len(rec_cluster_mask) > 0 else []\n",
    "        temporal_positions.extend(normalized_positions)\n",
    "    \n",
    "    if len(temporal_positions) > 0:\n",
    "        print(f\"  Mean position in recording: {np.mean(temporal_positions):.2f} (0=start, 1=end)\")\n",
    "        print(f\"  Std position: {np.std(temporal_positions):.2f}\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(temporal_positions, bins=20, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Normalized Position in Recording')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Cluster {cluster_id} - Temporal Distribution within Recordings')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "def compare_clusters(cluster_id1, cluster_id2):\n",
    "    \"\"\"Compare two clusters to understand their differences\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPARING CLUSTER {cluster_id1} vs CLUSTER {cluster_id2}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Size comparison\n",
    "    size1 = cluster_sizes[cluster_id1]\n",
    "    size2 = cluster_sizes[cluster_id2]\n",
    "    print(f\"Cluster {cluster_id1}: {size1} chirps\")\n",
    "    print(f\"Cluster {cluster_id2}: {size2} chirps\")\n",
    "    \n",
    "    # Prevalence comparison\n",
    "    prev1 = len(cluster_to_recordings[cluster_id1])\n",
    "    prev2 = len(cluster_to_recordings[cluster_id2])\n",
    "    print(f\"\\nCluster {cluster_id1}: appears in {prev1} recordings\")\n",
    "    print(f\"Cluster {cluster_id2}: appears in {prev2} recordings\")\n",
    "    \n",
    "    # Overlap\n",
    "    recordings1 = cluster_to_recordings[cluster_id1]\n",
    "    recordings2 = cluster_to_recordings[cluster_id2]\n",
    "    overlap = len(recordings1 & recordings2)\n",
    "    print(f\"\\nRecordings with both clusters: {overlap}\")\n",
    "    print(f\"Overlap coefficient: {overlap / min(prev1, prev2):.3f}\")\n",
    "    \n",
    "    # Embedding distance\n",
    "    mask1 = cluster_labels == cluster_id1\n",
    "    mask2 = cluster_labels == cluster_id2\n",
    "    centroid1 = all_embeddings_concat[mask1].mean(axis=0)\n",
    "    centroid2 = all_embeddings_concat[mask2].mean(axis=0)\n",
    "    distance = np.linalg.norm(centroid1 - centroid2)\n",
    "    print(f\"\\nEmbedding space distance between centroids: {distance:.3f}\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER EXPLORATION UTILITIES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"1. visualize_cluster_embeddings(cluster_id, method='tsne')\")\n",
    "print(\"   - Visualize chirps in a cluster using dimensionality reduction\")\n",
    "print()\n",
    "print(\"2. get_cluster_statistics(cluster_id)\")\n",
    "print(\"   - Get detailed statistics about a specific cluster\")\n",
    "print()\n",
    "print(\"3. compare_clusters(cluster_id1, cluster_id2)\")\n",
    "print(\"   - Compare two clusters to understand their differences\")\n",
    "print()\n",
    "print(\"Example usage:\")\n",
    "print(\"  get_cluster_statistics(0)  # Explore cluster 0\")\n",
    "print(\"  visualize_cluster_embeddings(0)  # Visualize cluster 0\")\n",
    "print(\"  compare_clusters(0, 1)  # Compare clusters 0 and 1\")\n",
    "print()\n",
    "\n",
    "# Automatically explore the top 3 clusters\n",
    "if len(cluster_sizes_sorted) >= 3:\n",
    "    print(\"\\nAutomatically exploring top 3 largest clusters...\")\n",
    "    for i in range(3):\n",
    "        cluster_id = cluster_sizes_sorted[i][0]\n",
    "        get_cluster_statistics(cluster_id)\n",
    "        print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BatGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
